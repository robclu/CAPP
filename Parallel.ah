/*
 * Aspect that is used to setup the OpenCl
 * components of a parallel class. This essentially 
 * abstracts the OpenCL setup away from the class and 
 * allows it to behave like normal x86 code.
 * */

#ifndef __PARALLEL__
#define __PARALLEL__

#include <stdio.h>
#include <iostream>
#include <fstream>
#include <iterator>
#include <utility>
#include <string>

#ifdef __APPLE__
#include <CL/opencl.hpp>
#else
#include <CL/cl.hpp>
#endif

#define T	 float   
#define This tjp->that()		// Pointer to the joinpoint instance

using namespace std;

aspect Parallel 
{
	public:
			
		// OpenCL program name on which to apply aspect
		pointcut programs() = "VectorAddParallel";	

		// Scope for the programs 
		pointcut scope() = "% ...::VectorAddParallel::%(...)";

		// Sets up opencl context in target class
		pointcut cl_setup() = construction(programs());

		// Pointcut to modify the kernel execution
		pointcut kernel_setup() = construction(programs());

		// Pointcut used to simplify running the kernel
		pointcut kernel_run() = execution("% ...::runKernel(...)");
		
		// Class that will hold the OpenCL variables
		slice class cl_variables
		{
			private:

				// Platforms avaialable
				vector<cl::Platform> platforms;

				// Context for OpenCL
				cl::Context context;

				// Devices available for the platform 
				vector<cl::Device> devices;

				// Command queue for OpenCL
				cl::CommandQueue queue;

				// Kernel for device execution
				cl::Kernel kernel;

				// CL Program variable 
				cl::Program program;

				// Buffers that move memory between host and device 
				vector<cl::Buffer> buffers;	
		};

		// Add the opencl variables to the class
		advice programs() : slice cl_variables;

		// Setup of the OpenCl environment 
		advice cl_setup() : around() 
		{
			// Get the available opencl platforms 
			cl::Platform::get(&(tjp->that()->platforms));
		
			// Create relevant context 
			if (tjp->that()->device_type == "GPU") {
				cl_context_properties ctext_properties[3] = {
					CL_CONTEXT_PLATFORM,
					(cl_context_properties)(tjp->that()->platforms[0])(), 0
				};
				tjp->that()->context = cl::Context(CL_DEVICE_TYPE_GPU, ctext_properties); 
			}
			else if (tjp->that()->device_type == "CPU" && tjp->that()->platforms.size() > 1) {
				cl_context_properties ctext_properties[3] = {
					CL_CONTEXT_PLATFORM,
					(cl_context_properties)(tjp->that()->platforms[1])(), 0					
				};
				tjp->that()->context = cl::Context(CL_DEVICE_TYPE_CPU, ctext_properties);
			}
			else {
				// One platform then the default type must be used 
				cl_context_properties ctext_properties[3] = {
					CL_CONTEXT_PLATFORM,
					(cl_context_properties)(tjp->that()->platforms[0])(),
					0
				};
				tjp->that()->context = cl::Context(CL_DEVICE_TYPE_DEFAULT, ctext_properties);
			}

			// Get devices for the chosen platform 
			tjp->that()->devices = tjp->that()->context.cl::Context::getInfo<CL_CONTEXT_DEVICES>();

			// Create command queue if a device is available
			// and use the default device (first available)
			if (tjp->that()->devices.size() > 0) {
				tjp->that()->queue = cl::CommandQueue(tjp->that()->context, tjp->that()->devices[0]);
			}

			tjp->proceed();
		}

		// Setup a kernel for the parallel program
		advice kernel_setup() : after() 
		{
			ifstream kernel_source(tjp->that()->kernel_source.c_str());	

			// Convert the kernel source to a string 
			string source_string(istreambuf_iterator<char>(kernel_source),
								 (istreambuf_iterator<char>()));

			// Create a cl source 
			cl::Program::Sources source(1, make_pair(source_string.c_str(), source_string.length() + 1));

			// Create OpenCL program from context and source and build it 
			tjp->that()->program = cl::Program(tjp->that()->context, source);
			tjp->that()->program.build(tjp->that()->devices);

			// Make the kernel 
			tjp->that()->kernel = cl::Kernel(tjp->that()->program, tjp->that()->kernel_name.c_str());
		}

		// Take care of the ugly OpenCL memory movement 
		advice kernel_run() : before()
		{

			vector< vector<T> > * inputs  = tjp->arg<0>();			// Arg(0) of runKernel(..) - kernel inputs
			vector< vector<T> > * outputs = tjp->arg<1>();			// Arg(1) of runKernel(..) - kernel outputs

			// Create the input buffers
			for (size_t i = 0; i < inputs->size(); i++) {
				// Create a buffer
				This->buffers.push_back(cl::Buffer(This->context, CL_MEM_READ_ONLY, (*inputs)[i].size() * sizeof(T)));
				// Copy data from input vector (host) to device buffer (device)
				This->queue.enqueueWriteBuffer(This->buffers.back(), CL_TRUE, 0, (*inputs)[i].size() * sizeof(T), &((*inputs)[i][0]));
			}

			// Create the output buffers
			for (size_t i = 0; i < outputs->size(); i++) {
				This->buffers.push_back(cl::Buffer(This->context, CL_MEM_WRITE_ONLY, (*outputs)[i].size() * sizeof(T)));
			}

			// Set the kernel arguments
			for (size_t i = 0; i < This->buffers.size(); i++) {
				This->kernel.setArg(i, This->buffers[i]);
			}

			// Set the work group dimensions
			cl::NDRange global((*inputs)[0].size());
			cl::NDRange local(1);					

			// Place the kernel on the command queue
			This->queue.enqueueNDRangeKernel(This->kernel, cl::NullRange, global, local);

			// Get the results back to the output vecto
			for (size_t i = 0; i < outputs->size(); i++) {
				// Create a buffer for the results from the kernel
				T * results = new T[(*outputs)[0].size()];

				// Move the result from the device to the host
				This->queue.enqueueReadBuffer(This->buffers[inputs->size() + i], CL_TRUE, 0, (*outputs)[i].size() * sizeof(T), results);

				// Put the result into the output vector
				(*outputs)[i] = vector<T>(results, results + (*outputs)[i].size());
				free(results);
			}
		}
};

#endif
